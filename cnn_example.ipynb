{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "\treturn 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Derivative of the sigmoid activation function\n",
    "def sigmoid_prime(z):\n",
    "\treturn sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forward Convolution\n",
    "def conv_forward(X, W):\n",
    "    '''\n",
    "    The forward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    X -- output activations of the previous layer, numpy array of shape (n_H_prev, n_W_prev) assuming input channels = 1\n",
    "    W -- Weights, numpy array of size (f, f) assuming number of filters = 1\n",
    "    \n",
    "    Returns:\n",
    "    H -- conv output, numpy array of size (n_H, n_W)\n",
    "    cache -- cache of values needed for conv_backward() function\n",
    "    '''\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Compute the output dimensions assuming no padding and stride = 1\n",
    "    n_H = n_H_prev - f + 1\n",
    "    n_W = n_W_prev - f + 1\n",
    "    \n",
    "    # Initialize the output H with zeros\n",
    "    H = np.zeros((n_H, n_W))\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of output volume\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            x_slice = X[h:h+f, w:w+f]\n",
    "            H[h,w] = np.sum(x_slice * W)\n",
    "            \n",
    "    # Saving information in 'cache' for backprop\n",
    "    cache = (X, W)\n",
    "    \n",
    "    return H, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Backward Convolution\n",
    "def conv_backward(dH, cache):\n",
    "    '''\n",
    "    The backward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
    "    '''\n",
    "    \n",
    "    # Retrieving information from the \"cache\"\n",
    "    (X, W) = cache\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Retrieving dimensions from dH's shape\n",
    "    (n_H, n_W) = dH.shape\n",
    "    \n",
    "    # Initializing dX, dW with the correct shapes\n",
    "    dX = np.zeros(X.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            dX[h:h+f, w:w+f] += W * dH[h,w]\n",
    "            dW += X[h:h+f, w:w+f] * dH[h,w]\n",
    "    \n",
    "    return dX, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input data\n",
    "X = np.asarray([[0.1,0.2,1.3],[0.37,1.09,1.02],[0.12,-0.23,-0.45]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1 ,  0.2 ,  1.3 ],\n",
       "       [ 0.37,  1.09,  1.02],\n",
       "       [ 0.12, -0.23, -0.45]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights\n",
    "W = np.asarray([[0.4,-0.1],[0.23,0.61]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4 , -0.1 ],\n",
       "       [ 0.23,  0.61]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward convolution\n",
    "Z, data_cache = conv_forward(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.1 ,  0.2 ,  1.3 ],\n",
       "        [ 0.37,  1.09,  1.02],\n",
       "        [ 0.12, -0.23, -0.45]]), array([[ 0.4 , -0.1 ],\n",
       "        [ 0.23,  0.61]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77  ,  0.8229],\n",
       "       [-0.0737,  0.0066]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation\n",
    "H = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68352089,  0.69485158],\n",
       "       [ 0.48158334,  0.50164999]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dH = sigmoid_prime(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21632008,  0.21203286],\n",
       "       [ 0.24966083,  0.24999728]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward convolution\n",
    "dX, dW = conv_backward(dH, data_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42891012,  0.84603426],\n",
       "       [ 0.28361417,  0.28214164]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08652803,  0.06318114, -0.02120329],\n",
       "       [ 0.14961795,  0.25575564,  0.10434032],\n",
       "       [ 0.05742199,  0.20979248,  0.15249834]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
